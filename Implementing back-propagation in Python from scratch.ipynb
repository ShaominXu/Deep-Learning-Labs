{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaominXu/Deep-Learning-Labs/blob/main/Implementing%20back-propagation%20in%20Python%20from%20scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKRRjmhbFcx7"
      },
      "source": [
        "In this problem we will train a neural network from scratch using numpy. In practice, you will never need to do this (you'd just use TensorFlow or PyTorch). But hopefully this will give us a sense of what's happening under the hood.\n",
        "\n",
        "For training/testing, we will use the standard MNIST benchmark consisting of images of handwritten images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjvPSnDA4J_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b8b754de-4c9b-45e3-8065-453b1cf3386b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "\n",
        "plt.imshow(x_train[0],cmap='gray');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klx9qPmxF9jI"
      },
      "source": [
        "Loading MNIST is the only place where we will use TensorFlow; the rest of the code will be pure numpy.\n",
        "\n",
        "Let us now set up a few helper functions. We will use sigmoid activations for neurons, the softmax activation for the last layer, and the cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdyvaUKoF7ux"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Numerically stable sigmoid function based on\n",
        "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
        "\n",
        "  x = np.clip(x, -500, 500) # We get an overflow warning without this\n",
        "\n",
        "  return np.where(\n",
        "    x >= 0,\n",
        "    1 / (1 + np.exp(-x)),\n",
        "    np.exp(x) / (1 + np.exp(x))\n",
        "  )\n",
        "\n",
        "def dsigmoid(x): # Derivative of sigmoid\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Numerically stable softmax based on (same source as sigmoid)\n",
        "  # http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
        "  b = x.max()\n",
        "  y = np.exp(x - b)\n",
        "  return y / y.sum()\n",
        "\n",
        "def cross_entropy_loss(y, yHat):\n",
        "  return -np.sum(y * np.log(yHat))\n",
        "\n",
        "def integer_to_one_hot(x, max):\n",
        "  # x: integer to convert to one hot encoding\n",
        "  # max: the size of the one hot encoded array\n",
        "  result = np.zeros(10)\n",
        "  result[x] = 1\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIZEupTHyNM"
      },
      "source": [
        "OK, we are now ready to build and train our model. The input is an image of size 28x28, and the output is one of 10 classes. So, first:\n",
        "\n",
        "Q1. Initialize a 2-hidden layer neural network with 32 neurons in each hidden layer, i.e., your layer sizes should be:\n",
        "\n",
        "784 -> 32 -> 32 -> 10\n",
        "\n",
        "If the layer is $n_{in} \\times n_{out}$ your layer weights should be initialized by sampling from a normal distribution with mean zero and variance 1/$\\max(n_{in},n_{out})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBeGvbu6FaM_"
      },
      "source": [
        "import math\n",
        "\n",
        "# Initialize weights of each layer with a normal distribution of mean 0 and\n",
        "# standard deviation 1/sqrt(n), where n is the number of inputs.\n",
        "# This means the weighted input will be a random variable itself with mean\n",
        "# 0 and standard deviation close to 1 (if biases are initialized as 0, standard\n",
        "# deviation will be exactly 1)\n",
        "\n",
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng(80085)\n",
        "\n",
        "# Q1. Fill initialization code here.\n",
        "\n",
        "LayerSizes = [784, 32, 32, 10]\n",
        "Weights1 = rng.normal(0,1/np.sqrt(LayerSizes[0]),size=(LayerSizes[0],LayerSizes[1]))\n",
        "Weights2 = rng.normal(0,1/np.sqrt(LayerSizes[1]),size=(LayerSizes[1],LayerSizes[2]))\n",
        "Weights3 = rng.normal(0,1/np.sqrt(LayerSizes[2]),size=(LayerSizes[2],LayerSizes[3]))\n",
        "\n",
        "Biases1 = np.zeros((1,LayerSizes[1]))\n",
        "Biases2 = np.zeros((1,LayerSizes[2]))\n",
        "Biases3 = np.zeros((1,LayerSizes[3]))\n",
        "\n",
        "#weights = [Weights1,Weights2,Weights3]\n",
        "biases = [Biases1,Biases2,Biases3]\n",
        "\n",
        "weights = [\n",
        " rng.normal(0, 1/math.sqrt(784), (32, 784)),\n",
        " rng.normal(0, 1/math.sqrt(32), (32, 32)),\n",
        " rng.normal(0, 1/math.sqrt(32), (10, 32))\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IafUGD_VGeLh"
      },
      "source": [
        "Next, we will set up the forward pass. We will implement this by looping over the layers and successively computing the activations of each layer.\n",
        "\n",
        "Q2. Implement the forward pass for a single sample, and for the entire dataset.\n",
        "\n",
        "\n",
        "Right now, your network weights should be random, so doing a forward pass with the data should not give you any meaningful information. Therefore, in the last line, when you calculate test accuracy, it should be somewhere around 1/10 (i.e., a random guess)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd6jGroQGdOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "dfec46b3-c67d-401a-b8c8-910383b90bed"
      },
      "source": [
        "def feed_forward_sample(sample, y):\n",
        "  \"\"\" Forward pass through the neural network.\n",
        "    Inputs:\n",
        "      sample: 1D numpy array. The input sample (an MNIST digit).\n",
        "      label: An integer from 0 to 9.\n",
        "\n",
        "    Returns: the cross entropy loss, most likely class\n",
        "  \"\"\"\n",
        "  # Q2. Fill code here.\n",
        "  # ...\n",
        "  numclasses = 10\n",
        "  u1 = np.dot(sample,weights[0]) + biases[0]\n",
        "  z1 = sigmoid(u1)\n",
        "  u2 = np.dot(z1,weights[1]) + biases[1]\n",
        "  z2 = sigmoid(u2)\n",
        "  u3 = np.dot(z2,weights[2]) + biases[2]\n",
        "  yhat = softmax(u3)\n",
        "  pred = np.argmax(yhat)\n",
        "  one_hot_guess = integer_to_one_hot(pred,numclasses)\n",
        "\n",
        "  yvector = integer_to_one_hot(y,numclasses)\n",
        "  loss = cross_entropy_loss(yvector,yhat)\n",
        "\n",
        "  return loss, one_hot_guess\n",
        "\n",
        "\n",
        "def feed_forward_dataset(x, y):\n",
        "  losses = np.empty(x.shape[0])\n",
        "  one_hot_guesses = np.empty((x.shape[0], 10))\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    sample = np.reshape(x[i],(1,784))\n",
        "    [loss_sample,one_hot_guess_sample] = feed_forward_sample(sample,y[i])\n",
        "    losses[i] = loss_sample\n",
        "    one_hot_guesses[i] = one_hot_guess_sample\n",
        "\n",
        "  y_one_hot = np.zeros((y.size, 10))\n",
        "  y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "  correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
        "  correct_guess_percent = format((correct_guesses / y.shape[0]) * 100, \".2f\")\n",
        "\n",
        "  print(\"\\nAverage loss:\", np.round(np.average(losses), decimals=2))\n",
        "  print(\"Accuracy (# of correct guesses):\", correct_guesses, \"/\", y.shape[0], \"(\", correct_guess_percent, \"%)\")\n",
        "\n",
        "def feed_forward_training_data():\n",
        "  print(\"Feeding forward all training data...\")\n",
        "  feed_forward_dataset(x_train, y_train)\n",
        "  print(\"\")\n",
        "\n",
        "def feed_forward_test_data():\n",
        "  print(\"Feeding forward all test data...\")\n",
        "  feed_forward_dataset(x_test, y_test)\n",
        "  print(\"\")\n",
        "\n",
        "feed_forward_test_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feeding forward all test data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-92b1e08e6af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mfeed_forward_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-92b1e08e6af8>\u001b[0m in \u001b[0;36mfeed_forward_test_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feeding forward all test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mfeed_forward_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-92b1e08e6af8>\u001b[0m in \u001b[0;36mfeed_forward_dataset\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mloss_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_guess_sample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_forward_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mone_hot_guesses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_guess_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-92b1e08e6af8>\u001b[0m in \u001b[0;36mfeed_forward_sample\u001b[0;34m(sample, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mnumclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,784) and (32,784) not aligned: 784 (dim 1) != 32 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrlc2VLOi8L"
      },
      "source": [
        "OK, now we will implement the backward pass using backpropagation. We will keep it simple and just do training sample-by-sample (no minibatching, no randomness).\n",
        "\n",
        "Q3: Compute the gradient of all the weights and biases by backpropagating derivatives all the way from the output to the first layer.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLLEsVdcOgzi"
      },
      "source": [
        "def train_one_sample(sample, y, learning_rate=0.003):\n",
        "  a = np.reshape(sample,(1,28*28))\n",
        "\n",
        "  # We will store each layer's activations to calculate gradient\n",
        "  activations = []\n",
        "  num_layers = 3\n",
        "  weight_gradients = []\n",
        "  bias_gradients = []\n",
        "\n",
        "  # Forward pass\n",
        "  # (same as what we did above, but keep track of intermediate activations)\n",
        "\n",
        "  numclasses = 10\n",
        "  u1 = np.dot(a,weights[0]) + biases[0]\n",
        "  z1 = sigmoid(u1)\n",
        "  activations.append(z1)\n",
        "  u2 = np.dot(z1,weights[1]) + biases[1]\n",
        "  z2 = sigmoid(u2)\n",
        "  activations.append(z2)\n",
        "  u3 = np.dot(z2,weights[2]) + biases[2]\n",
        "  yhat = softmax(u3)\n",
        "  activations.append(yhat)\n",
        "  pred = np.argmax(yhat)\n",
        "  one_hot_guess = integer_to_one_hot(pred,numclasses)\n",
        "  yvector = integer_to_one_hot(y,numclasses)\n",
        "  loss = cross_entropy_loss(yvector,yhat)\n",
        "\n",
        "  # Backward pass\n",
        "  du3 = activations[2]-yvector #a[2]=y_hat\n",
        "  # dL_dW3 = du3_dW3*dL_du3\n",
        "  dW3 = activations[1].T.dot(du3) #a[1]=z2 # dL_db3 = du3_db3*dL_du3\n",
        "  db3 = du3\n",
        "  # dL_dz2 = dL_du3*du3_dz2\n",
        "  dz2 = du3.dot(weights[2].T)\n",
        "  # dL_dW2 = dz2_dW2*dL_dz2\n",
        "  dW2 = activations[0].T.dot(np.multiply(dz2,dsigmoid(u2))) #a[0] = z1 # dL_db2 = dz2_db2*dL_dz2\n",
        "  db2 = np.multiply(dz2,dsigmoid(u2))\n",
        "  # dL_dz1 = dL_dz2*dz2_dz1\n",
        "  dz1 = np.multiply(dz2.dot(weights[1].T),dsigmoid(u2))\n",
        "  # dz1_dW1 = dL_dz1*dz1_dW1\n",
        "  dW1 = a.T.dot(np.multiply(dz1,dsigmoid(u1)))\n",
        "  # dz1_db1 = dL_dz1*dz1_db1\n",
        "  db1 = np.multiply(dz1,dsigmoid(u1))\n",
        "\n",
        "  # Assemble gradients\n",
        "  weight_gradients.append(dW1)\n",
        "  weight_gradients.append(dW2)\n",
        "  weight_gradients.append(dW3)\n",
        "  bias_gradients.append(db1)\n",
        "  bias_gradients.append(db2)\n",
        "  bias_gradients.append(db3)\n",
        "\n",
        "  # Update weights & biases based on your calculated gradient\n",
        "  for i in range(num_layers):\n",
        "    weights[i] -= weight_gradients[i] * learning_rate\n",
        "    biases[i] -= bias_gradients[i].flatten() * learning_rate\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AXY27pOB9cW"
      },
      "source": [
        "Finally, train for 3 epochs by looping over the entire training dataset 3 times.\n",
        "\n",
        "Q4. Train your model for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ygk05FcB-rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9979691-db04-422e-fdab-3ac851e6e558"
      },
      "source": [
        "def train_one_epoch(learning_rate=0.005):\n",
        "  print(\"Training for one epoch over the training dataset...\")\n",
        "\n",
        "  for i in range(x_train.shape[0]):\n",
        "    train_one_sample(x_train[i], y_train[i], learning_rate)\n",
        "\n",
        "  print(\"Finished training.\\n\")\n",
        "\n",
        "\n",
        "feed_forward_test_data()\n",
        "\n",
        "def test_and_train():\n",
        "  train_one_epoch()\n",
        "  feed_forward_test_data()\n",
        "\n",
        "for i in range(3):\n",
        "  test_and_train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feeding forward all test data...\n",
            "\n",
            "Average loss: 2.37\n",
            "Accuracy (# of correct guesses): 880.0 / 10000 ( 8.80 %)\n",
            "\n",
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Feeding forward all test data...\n",
            "\n",
            "Average loss: 1.21\n",
            "Accuracy (# of correct guesses): 5488.0 / 10000 ( 54.88 %)\n",
            "\n",
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Feeding forward all test data...\n",
            "\n",
            "Average loss: 1.22\n",
            "Accuracy (# of correct guesses): 5791.0 / 10000 ( 57.91 %)\n",
            "\n",
            "Training for one epoch over the training dataset...\n",
            "Finished training.\n",
            "\n",
            "Feeding forward all test data...\n",
            "\n",
            "Average loss: 1.2\n",
            "Accuracy (# of correct guesses): 5754.0 / 10000 ( 57.54 %)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKzEn_lyCAIe"
      },
      "source": [
        "\n",
        "That's it!\n",
        "\n",
        "Your code is probably very time- and memory-inefficient; that's ok. There is a ton of optimization under the hood in professional deep learning frameworks which we won't get into.\n",
        "\n",
        "If everything is working well, you should be able to raise the accuracy from ~10% to ~70% accuracy after 3 epochs."
      ]
    }
  ]
}